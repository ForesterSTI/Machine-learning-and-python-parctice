{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imprting Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amosm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore', category=Warning, lineno=0, append=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procssing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load text data \n",
    "text_data ={\n",
    "     \"Wow... Loved this place.\",\n",
    "    \"Crust is not good.\",\n",
    "    \"Not tasty and the texture was just nasty.\",\n",
    "    \"Stopped by during the late May bank holiday of...\",\n",
    "    \"The selection on the menu was great and so wer...\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Crust', 'is', 'not', 'good', '.'],\n",
       " ['The',\n",
       "  'selection',\n",
       "  'on',\n",
       "  'the',\n",
       "  'menu',\n",
       "  'was',\n",
       "  'great',\n",
       "  'and',\n",
       "  'so',\n",
       "  'wer',\n",
       "  '...'],\n",
       " ['Stopped',\n",
       "  'by',\n",
       "  'during',\n",
       "  'the',\n",
       "  'late',\n",
       "  'May',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'of',\n",
       "  '...'],\n",
       " ['Not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty', '.'],\n",
       " ['Wow', '...', 'Loved', 'this', 'place', '.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we turn text into a tolenised LIST of data\n",
    "tokenize_data = [nltk.word_tokenize(text) for text in text_data]\n",
    "tokenize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lowe =[text.lower() for text in text_data()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
